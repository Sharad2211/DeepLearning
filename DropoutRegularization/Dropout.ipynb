import tensorflow as tf
import numpy as np
from tensorflow import keras
from matplotlib import pyplot
imdbDB = keras.datasets.imdb
(train_data,train_labels),(test_data,test_labels)=imdbDB.load_data(num_words=10000)
print (f"Training entries {len(train_data)}. Labels: {train_labels}")
print(train_data[0])
print(test_data[0])
word_index = imdbDB.get_word_index()

#add more words to the index
word_index = {k:(v+3) for k, v in word_index.items()}

word_index["<PAD>"]=0
word_index["<START>"]=1
word_index["<UNK>"]=2
word_index["<UNUSED>"]=3

#creating a value to key dictionary - reverse of the word index
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])



def decode_review (text):
  return ' '.join([reverse_word_index.get(i,"?") for i in text])

print(decode_review(train_data[0]))

train_data = keras.preprocessing.sequence.pad_sequences(train_data, 
                                                        value = word_index["<PAD>"],
                                                        padding="post",
                                                        maxlen=256)
test_data = keras.preprocessing.sequence.pad_sequences(test_data, 
                                                       value = word_index["<PAD>"],
                                                       padding="post",
                                                       maxlen=256)

vocab_size = 10000
model = keras.Sequential()
model.add(keras.layers.Embedding(vocab_size,16)) # input layer
model.add(keras.layers.GlobalAvgPool1D())
model.add(keras.layers.Dense(16,activation=tf.nn.relu))  #hidden layer1
model.add(keras.layers.Dropout(0.5, noise_shape=None, seed=None))
model.add(keras.layers.Dense(16,activation=tf.nn.relu))  #hidden layer2
model.add(keras.layers.Dropout(0.4, noise_shape=None, seed=None)) 
model.add(keras.layers.Dense(16,activation=tf.nn.relu))  #hidden layer3
model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None)) 
model.add(keras.layers.Dense(16,activation=tf.nn.relu))  #hidden layer4
model.add(keras.layers.Dropout(0.2, noise_shape=None, seed=None)) 
model.add(keras.layers.Dense(16,activation=tf.nn.relu))  #hidden layer5 
model.add(keras.layers.Dense(16,activation=tf.nn.relu))  #hidden layer6                      
model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid)) #output layer


# REGULARIZERS L1 L2
#regularizers.l1(0.001)
#regularizers.l2(0.001)
#regularizers.l1_l2(l1=0.001, l2=0.001)

model.summary()

model.compile(optimizer ="adam", loss="binary_crossentropy", metrics=["acc"])

#checking the accuracy of the model using the 10000 train data - validation sets
x_val=train_data [:10000]
partial_x_train = train_data[10000:]

y_val=train_labels [:10000]
partial_y_train = train_labels[10000:]

history= model.fit(partial_x_train,partial_y_train,epochs=40,batch_size=512,validation_data=(x_val,y_val),verbose=1)

#history_dict = history.history
#print (history_dict.keys() )
#results = model.evaluate(test_data,test_labels)
#print(results

_, train_acc = model.evaluate(partial_x_train, partial_y_train, verbose=0)
_, test_acc = model.evaluate(x_val, y_val, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
# plot history
pyplot.plot(history.history['acc'], label='train')
pyplot.plot(history.history['val_acc'], label='test')
pyplot.legend()
pyplot.show()




# PREDICT

# Texts
#text_bad = train_data[7737]
#text_good = train_data[449]
#texts = (text_bad, text_good)

#print("bad")
#print(decode_review(train_data[7737]))
#print("good")
#print(decode_review(train_data[449]))
#print ("padded texts")
#print (padded_texts)
# Generate predictions
#predictions = model.predict(padded_texts)
